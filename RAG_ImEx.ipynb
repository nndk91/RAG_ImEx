{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f280f1a-3f0c-4019-b132-795d2cc23276",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q python-dotenv\n",
    "!pip install -q gradio\n",
    "!pip install -q -U langchain-community\n",
    "!pip install -q unstructured\n",
    "!pip install -q openpyxl\n",
    "!pip install -q tiktoken\n",
    "!pip install -q chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a891017b-6b58-4b5f-8ddc-1f172a8e8030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import chromadb\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "#from google.colab import userdata\n",
    "from chromadb import chromadb\n",
    "from sklearn.manifold import TSNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edefd60f-454f-4082-9b43-6aa9da0411a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader, UnstructuredExcelLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d4c8739-b5c3-4929-bb0b-85f631b0405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f56e22-aff0-4c9b-b4e6-1e4ba96e9eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in google Colab\n",
    "# openai = userdata.get('OPENAI_API_KEY')\n",
    "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98574e4d-7db2-4755-9c05-4b61061628ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42b3ab33-0c35-408c-82e1-b9fd0de21b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge base is in /RAG_ImEx/Data\n",
    "knowledge_base_path = \"Data/*\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e565f-46f2-41de-8b8d-e3badd060f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load all Excel sheets in the knowledge base path using glob\n",
    "all_dfs = []\n",
    "excel_files = glob.glob(knowledge_base_path) # Use glob for wildcard matching\n",
    "\n",
    "if not excel_files:  # Check if any Excel files were found\n",
    "    print(f\"No Excel files found in '{knowledge_base_path}'.\")\n",
    "else:\n",
    "    for filepath in excel_files:\n",
    "        try: #Add try except block to handle potential errors during file reading\n",
    "            df = pd.read_excel(filepath, header=0)\n",
    "            all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file '{filepath}': {e}\")\n",
    "\n",
    "    if all_dfs: # Check if any dataframes were successfully loaded\n",
    "        # Concatenate all dataframes into a single dataframe\n",
    "        combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        # Now you can work with the combined_df\n",
    "        print(f\"Successfully combined data from {len(excel_files)} Excel files.\")\n",
    "        # ... your code to process combined_df ...\n",
    "    else:\n",
    "        print(\"No dataframes could be loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37793b4d-9ec0-42fd-99af-1f3e4d710789",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Create documents\n",
    "documents = []\n",
    "for index, row in combined_df.iterrows():\n",
    "    text = ' '.join(row[combined_df.columns].astype(str))\n",
    "    metadata = {'row': index + 2, 'filename': filename}  # Add filename to metadata\n",
    "    doc = Document(page_content=text, metadata=metadata)\n",
    "    documents.append(doc)\n",
    "\n",
    "\n",
    "# 3. Create vector database\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a853f7a9-55fe-4c19-b3f6-fcadfd08f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's investigate the vectors\n",
    "\n",
    "collection = vectorstore._collection\n",
    "count = collection.count()\n",
    "\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"There are {count:,} vectors with {dimensions:,} dimensions in the vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf6831-9d7d-4457-a726-94b8914192c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with OpenAI\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 25})\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931a843e-e450-4234-94ab-f55acb861934",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"which company import cold rolled stainless steel?\"\n",
    "result = conversation_chain.invoke({\"question\":query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0097d1-0c23-4b72-a9e5-55ebcc78a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a new conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fcbf31-fda7-495d-b6eb-d2eb9938ce2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping in a function - note that history isn't used, as the memory is in the conversation_chain\n",
    "\n",
    "def chat(message, history):\n",
    "    result = conversation_chain.invoke({\"question\": message})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6461a4e0-2776-472f-93d2-46edfda8ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And in Gradio:\n",
    "\n",
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37253e2e-8768-4036-a0e9-5fb44f489c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da3eab0-7a4c-4999-af90-778bb500a095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
