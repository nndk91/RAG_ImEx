{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f280f1a-3f0c-4019-b132-795d2cc23276",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q python-dotenv\n",
    "!pip install -q gradio\n",
    "!pip install -q unstructured\n",
    "!pip install -q openpyxl\n",
    "!pip install -q tiktoken\n",
    "!pip install -q chromadb\n",
    "!pip install -q sentence-transformers\n",
    "!pip install -q --upgrade langchain\n",
    "!pip install -q -U langchain-chroma\n",
    "!pip install -q -U langchain-google-vertexai\n",
    "!pip install -q -U langchain-community\n",
    "!pip install -q -U langchain langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7355421a-70dc-4618-9c1e-c96e17dae235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a891017b-6b58-4b5f-8ddc-1f172a8e8030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import chromadb\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from chromadb import chromadb\n",
    "from sklearn.manifold import TSNE\n",
    "from datetime import datetime\n",
    "#from google.colab import userdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edefd60f-454f-4082-9b43-6aa9da0411a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader, UnstructuredExcelLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings, SentenceTransformerEmbeddings, HuggingFaceEmbeddings \n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_vertexai import ChatVertexAI #Import ChatVertexAI from langchain_google_vertexai\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from chromadb import Client, Settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d4c8739-b5c3-4929-bb0b-85f631b0405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76d8404f-8689-4cdc-8ea6-09c9105d72d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "Gemini_model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "MODEL = \"gemini-1.5-flash\" \n",
    "#Testing Gemini AI\n",
    "#response = model.generate_content(\"Explain how AI works\")\n",
    "#print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "154ee19e-b3d1-4fd3-b1fd-327b8da017e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI with your Project ID and location\n",
    "PROJECT_ID = \"gen-lang-client-0840327518\"  # Replace with your actual Project ID\n",
    "LOCATION = \"asia-southeast1\" #Replace with your location\n",
    "from vertexai import init\n",
    "init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# Define a base language model (if you haven't already)\n",
    "class _LanguageModel(BaseModel):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10f56e22-aff0-4c9b-b4e6-1e4ba96e9eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in google Colab\n",
    "# openai = userdata.get('OPENAI_API_KEY')\n",
    "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98574e4d-7db2-4755-9c05-4b61061628ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42b3ab33-0c35-408c-82e1-b9fd0de21b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge base is in /RAG_ImEx/Data\n",
    "#knowledge_base_path = \"Data/*\"  \n",
    "# Knowledge base is in /RAG_ImEx/Data\n",
    "knowledge_base_path = \"Data_test/*\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d48e5e19-139e-472a-843e-c9f722526ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Timestamp Handling ***\n",
    "def convert_timestamps(cell):\n",
    "    \"\"\"Converts Excel timestamps (float or datetime) to ISO 8601 strings.\"\"\"\n",
    "    if isinstance(cell, (float, int)):  # Check for numeric timestamps\n",
    "        try:\n",
    "            return pd.Timestamp(cell, unit='D').isoformat()  # Convert to datetime and then ISO string\n",
    "        except ValueError:\n",
    "            return cell #If the cell is not a valid timestamp, return the original value\n",
    "    elif isinstance(cell, datetime): #Check if the cell is already in datetime format\n",
    "        return cell.isoformat() #Return the ISO format of the datetime\n",
    "    return cell  # Return other cell types unchanged\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be25157c-780c-4d29-ba90-25ff10f4742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_excel_files(knowledge_base_path, columns_to_remove=None, address_cols=None, header_translations=None):\n",
    "    \"\"\"\n",
    "    Combines multiple Excel files into a single JSON string, handling variations in\n",
    "    column positions and optional column removal/address merging.\n",
    "\n",
    "    Args:\n",
    "        knowledge_base_path: A glob pattern (e.g., \"path/to/files/*.xlsx\").\n",
    "        columns_to_remove (optional): A list of column names to remove.\n",
    "        address_cols (optional): A list of address column names to merge.\n",
    "        header_translations (optional): Whether to merge address columns.\n",
    "    Returns:\n",
    "        A JSON string containing the combined data, or None if an error occurs.\n",
    "        Prints error messages to the console.\n",
    "    \"\"\"\n",
    "\n",
    "    excel_files = []\n",
    "    all_data = []\n",
    "\n",
    "    # Find all Excel files based on the provided path (can be a glob pattern)\n",
    "    initial_paths = glob.glob(knowledge_base_path)\n",
    "    \n",
    "    \n",
    "    for path in initial_paths:\n",
    "        if os.path.isdir(path):\n",
    "            excel_files.extend(glob.glob(os.path.join(path, \"*.xlsx\"))) # Add Excel files from subdirectories\n",
    "        elif os.path.isfile(path) and path.lower().endswith(('.xls', '.xlsx')):\n",
    "            excel_files.append(path)\n",
    "        else:\n",
    "            print(f\"Skipping: {path} (Not a directory or an Excel file)\")\n",
    "\n",
    "    if not excel_files:\n",
    "        print(f\"No Excel files found matching pattern: {knowledge_base_path}\")\n",
    "        return None\n",
    "\n",
    "    for file_path in excel_files:\n",
    "        try:\n",
    "            if os.path.isdir(file_path): # redundant check, already handled above.\n",
    "                print(f\"Skipping directory: {file_path}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            df = pd.read_excel(file_path)\n",
    "            \n",
    "            # Convert all relevant columns to string type *before* any string operations\n",
    "            for col in df.columns:\n",
    "                # Check if the column has mixed types or if it's numeric\n",
    "                if pd.api.types.is_numeric_dtype(df[col]) or not all(isinstance(x, str) or pd.isna(x) for x in df[col]):\n",
    "                    df[col] = df[col].astype(str)  # Convert the column to string type\n",
    "\n",
    "            # Convert column names to lowercase for consistency. Do this early.\n",
    "            df.columns = df.columns.str.lower()\n",
    "            if columns_to_remove:\n",
    "                columns_to_remove = [col.lower() for col in columns_to_remove]\n",
    "            if address_cols:\n",
    "                address_cols = [col.lower() for col in address_cols]\n",
    "            if header_translations:\n",
    "                header_translations = {k.lower(): v for k, v in header_translations.items()}\n",
    "            \n",
    "            # Merge address columns if specified and all columns are present.\n",
    "            if address_cols and all(col in df.columns for col in address_cols):\n",
    "                df['dia chi don vi doi tac'] = df[address_cols].apply(lambda row: ' '.join(row.dropna()), axis=1)\n",
    "                df = df.drop(address_cols, axis=1)\n",
    "            elif address_cols and not all(col in df.columns for col in address_cols):\n",
    "                print(f\"Warning: Not all address columns found in {file_path}. Skipping address merge.\")\n",
    "           \n",
    "            # Remove specified columns.\n",
    "            if columns_to_remove:\n",
    "                for col in columns_to_remove:\n",
    "                    try:\n",
    "                        df = df.drop(col, axis=1)\n",
    "                    except KeyError:\n",
    "                        print(f\"Warning: Column '{col}' not found in {file_path}. Skipping.\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"An error occurred during column removal in {file_path}: {e}\")\n",
    "                        return None  # Stop processing if a general exception occurs\n",
    "            \n",
    "            # header translations.\n",
    "            if header_translations:\n",
    "                try:\n",
    "                    df = df.rename(columns=header_translations)\n",
    "                except KeyError as e:  # Handle cases where translation key is not found\n",
    "                    print(f\"Error translating headers: Key '{e}' not found in DataFrame.\")\n",
    "                except Exception as e: # Catch other potential exceptions\n",
    "                    print(f\"An error occurred during header translation: {e}\")\n",
    "                    return None\n",
    "            \n",
    "            # Convert datetime columns to ISO format strings.\n",
    "            for col in df.columns:\n",
    "                if df[col].dtype == 'datetime64[ns]':\n",
    "                    df[col] = df[col].apply(lambda x: x.isoformat() if pd.notna(x) else None)\n",
    "\n",
    "            # Convert DataFrame to a list of dictionaries.\n",
    "            data = df.to_dict(orient='records')\n",
    "            '''\n",
    "            # Create a dictionary with metadata and the data.\n",
    "            document_meta = {\"doc_path\": file_path, \"doc_type\": \"DataBase\", \"text\": data}\n",
    "            '''\n",
    "            all_data.append(data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    #return json.dumps(all_data, ensure_ascii=False, indent=4)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a7cc08-5e50-48ff-b5b3-432c4116dfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = [\n",
    "    'MA HQ', 'MA HAI QUAN', 'MA CHI CUC',\n",
    "    'TEN HAI QUAN', 'SO DT', 'DIA DIEM CHO THONG QUAN',\n",
    "    'DIA DIEM CHO THONG QUAN',\n",
    "    'DIA DIEM DO HANG', 'DIA DIEM XEP HANG', 'P.TIEN V.CHUYEN', 'Tri GIA'\n",
    "]\n",
    "address_cols = ['dia chi 1', 'dia chi 2', 'dia chi 3', 'dia chi 4']\n",
    "\n",
    "header_translations = {\n",
    "    \"MA LH\": \"Import_Code\", \n",
    "    \"NHA NHAP KHAU\" : \"Cty nhập khẩu\",\n",
    "    \"DIA CHI\" : \"Địa chỉ cty nhập \",\n",
    "    \"DON VI DOI TAC\":\"Cty bán hàng\", \n",
    "    \"DIA CHI DON VI DOI TAC\" : \"Địa chỉ cty bán hàng\",\n",
    "    \"DK TT\": \"Hình Thức Thanh Toán\",\n",
    "    \"DK GH\": \"Incoterm\",\n",
    "    \"TI GIA\": \"Exchange_Rate\",\n",
    "    \"NGAY\" : \"Date\",\n",
    "    \"MA HANG\" : \"HSCODE\",\n",
    "    \"TEN HANG\": \"Product_Info\", \n",
    "    \"LUONG\": \"Quantity\",\n",
    "    \"DVT\" : \"Unit\",\n",
    "    \"DON GIA\" : \"Unit_Price\",\n",
    "    \"MA NT\": \"Payment_Currency\",\n",
    "    \"THUE XNK\" : \"Import_Tax\",\n",
    "    \"XUAT XU\" : \"Origin\", \n",
    "    \"MA DOANH NGHIEP\" : \"Tax ID\",\n",
    "   \n",
    "}\n",
    "combined_df = combine_excel_files(knowledge_base_path, columns_to_remove, address_cols, header_translations)\n",
    "\n",
    "if combined_df is not None:\n",
    "    print(\"Combined DataFrame:\")\n",
    "    print(combined_df)\n",
    "    #combined_df.to_excel(\"combined_file.xlsx\", index=False)  # Save to a new Excel file\n",
    "    #Save to a new Excel file (optional):\n",
    "    #processed_df.to_excel(\"processed_file.xlsx\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90db2ac-5e98-43ce-842b-9854e0624902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to LangChain Documents(this is the crucial step)\n",
    "langchain_documents = []\n",
    "for file_data in combined_df:  # Iterate through each list of dictionaries (each Excel file's data)\n",
    "    for doc in file_data: # Iterate through the dictionaries in the list. Each dict is one row.\n",
    "        metadata = {'source_file': 'DataBase',  'page_content' = \"\"} # Add metadata, if you have a doc_path\n",
    "        page_content\n",
    "        for key, value in doc.items():\n",
    "                page_content += f\"{key}: {value}\\n\"\n",
    "        langchain_doc = Document(page_content=page_content, metadata=metadata)\n",
    "        langchain_documents.append(langchain_doc)\n",
    "\n",
    "print(f\"Created {len(langchain_documents)} LangChain documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec9452b-9cda-4d02-9093-6170e84cb646",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f61ea8-4881-4d40-af04-04691a7f0c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(langchain_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8765a5-9395-470e-a7ed-53de5aafe3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db62cce-9cfe-4b21-ab07-e3eaf3bacfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b39e79-0a82-45dd-b081-457fdfab779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a suitable model.  'all-mpnet-base-v2' is a good general-purpose option.\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-mpnet-base-v2\")  # Or another Sentence Transformer model\n",
    "# Delete if already exists\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c28d07-48aa-40c4-a4c4-b445b438c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name) # Use the chunks (which are LangChain Documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a853f7a9-55fe-4c19-b3f6-fcadfd08f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's investigate the vectors\n",
    "\n",
    "collection = vectorstore._collection\n",
    "count = collection.count()\n",
    "\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"There are {count:,} vectors with {dimensions:,} dimensions in the vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2d769f-5377-4c7c-9eaf-dda523abe934",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_types = set(chunk.metadata['source_file'] for chunk in chunks)\n",
    "print(f\"Document types found: {', '.join(doc_types)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294bf4e1-4f5d-4d87-9343-c52fbd39f9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "results = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "print(results)\n",
    "'''\n",
    "results = collection.get(include=['embeddings'])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b7c68-5a04-4b21-ba1d-2947078b3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prework\n",
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "doc_types = [metadata['source_file'] for metadata in result['metadatas']]\n",
    "colors = [['blue'][['DataBase'].index(t)] for t in doc_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf6831-9d7d-4457-a726-94b8914192c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 3D!\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    z=reduced_vectors[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='3D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),\n",
    "    width=900,\n",
    "    height=700,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529dabee-8062-419e-bcb6-98f8abf92ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with Gemini\n",
    "#llm = ChatVertexAI(temperature=0.7, model_name=MODEL)\n",
    "# create a new Chat with OpenAI\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=\"gpt-4o-mini\")\n",
    "\n",
    "# Rebuild the model (this is the crucial missing step)\n",
    "ChatVertexAI.model_rebuild()\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 25})\n",
    "\n",
    "# Putting it together: set up the conversation chain with Gemini, \n",
    "# the vector store, and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, \n",
    "    retriever=retriever, \n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931a843e-e450-4234-94ab-f55acb861934",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what Samsung C&T importing?\"\n",
    "result = conversation_chain.invoke({\"question\":query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0097d1-0c23-4b72-a9e5-55ebcc78a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a new conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fcbf31-fda7-495d-b6eb-d2eb9938ce2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping in a function - note that history isn't used, as the memory is in the conversation_chain\n",
    "\n",
    "def chat(message, history):\n",
    "    result = conversation_chain.invoke({\"question\": message})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6461a4e0-2776-472f-93d2-46edfda8ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And in Gradio:\n",
    "\n",
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=False, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca058bc8-2555-4030-a9b5-36e05f1f0957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=\"gpt-4o-mini\")\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 25})\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory, callbacks=[StdOutCallbackHandler()])\n",
    "\n",
    "query = \"what VSSC import?\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "answer = result[\"answer\"]\n",
    "print(\"\\nAnswer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24accec-1566-432e-9353-c5a4d7b6673d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
