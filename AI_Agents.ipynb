{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bde428d-142d-44d3-96f8-b5824fa066b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q python-dotenv\n",
    "!pip install -q gradio\n",
    "!pip install -q unstructured\n",
    "!pip install -q openpyxl\n",
    "!pip install -q tiktoken\n",
    "!pip install -q chromadb\n",
    "!pip install -q sentence-transformers\n",
    "!pip install -q --upgrade langchain\n",
    "!pip install -q -U langchain-chroma\n",
    "!pip install -q -U langchain-google-vertexai\n",
    "!pip install -q -U langchain-community\n",
    "!pip install -q -U langchain langchain-huggingface\n",
    "!pip install -q sqlalchemy\n",
    "!pip install -q pymysql\n",
    "!pip install -q openai mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45246de-14da-4a92-9528-62fd938be717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bd8b97c2-cd82-4206-b1d1-8fb70009fb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import chromadb\n",
    "import mysql.connector\n",
    "import openai\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from chromadb import chromadb\n",
    "from sklearn.manifold import TSNE\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String, insert, Float, DateTime, inspect\n",
    "#from google.colab import userdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f27be11-fc8d-4dbb-a38d-3e44b7cd5466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader, UnstructuredExcelLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings, SentenceTransformerEmbeddings, HuggingFaceEmbeddings \n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_vertexai import ChatVertexAI #Import ChatVertexAI from langchain_google_vertexai\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from chromadb import Client, Settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b3556cb5-6799-41ec-8743-c23c98d942c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "load_dotenv()\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['MYSQL_KEY'] = os.getenv('MYSQL_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24825994-557c-447d-90a9-afeff74765b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "Gemini_model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "MODEL = \"gemini-1.5-flash\" \n",
    "#Testing Gemini AI\n",
    "#response = model.generate_content(\"Explain how AI works\")\n",
    "#print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afad22ce-31e0-41a2-9cf2-cc7ad55c014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edbb382d-7c7b-4b53-921f-443a07e5ac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge base is in /RAG_ImEx/Data\n",
    "#knowledge_base_path = \"Data/*\"  \n",
    "# Knowledge base is in /RAG_ImEx/Data_test\n",
    "knowledge_base_path = \"Data_test/*\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5bfad66d-baf4-4386-b2ce-0f4572a73003",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = [\n",
    "    'MA HQ', 'MA HAI QUAN', 'MA CHI CUC',\n",
    "    'TEN HAI QUAN', 'SO DT', 'DIA DIEM CHO THONG QUAN',\n",
    "    'DIA DIEM CHO THONG QUAN', 'DIA DIEM NHAN HANG',\n",
    "    'DIA DIEM DO HANG', 'DIA DIEM XEP HANG', 'P.TIEN V.CHUYEN', 'TRI GIA',\n",
    "    'NUOC NK','LOAI HINH'\n",
    "]\n",
    "address_cols = ['dia chi 1', 'dia chi 2', 'dia chi 3', 'dia chi 4']\n",
    "\n",
    "header_translations = {\n",
    "    \"MA LH\": \"Import_Code\", \n",
    "    \"NHA NHAP KHAU\" : \"Importer_Name\",\n",
    "    \"DIA CHI\" : \"Importer_Address\",\n",
    "    \"nha xuat khau\" : \"Exporter_Name\",\n",
    "    \"DIA CHI XK\" : \" Exporter_Address\",\n",
    "    \"DON VI DOI TAC\":\"Partner_Company\", \n",
    "    \"DIA CHI DON VI DOI TAC\" : \"Partner_Address\",\n",
    "    \"DK TT\": \"Payment_term\",\n",
    "    \"DK GH\": \"Incoterms\",\n",
    "    \"TI GIA\": \"Exchange_Rate\",\n",
    "    \"NGAY\" : \"Date\",\n",
    "    \"MA HANG\" : \"HSCODE\",\n",
    "    \"TEN HANG\": \"Product_Info\", \n",
    "    \"LUONG\": \"Quantity\",\n",
    "    \"DVT\" : \"Unit\",\n",
    "    \"DON GIA\" : \"Unit_Price\",\n",
    "    \"MA NT\": \"Payment_Currency\",\n",
    "    \"THUE XNK\" : \"Import_Tax\",\n",
    "    \"MA DOANH NGHIEP\" : \"Tax_ID\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e043ac7a-94f4-4fa3-9b55-7864628f657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_excel_files(folder_path, excel_type, columns_to_remove, address_cols, header_translations):\n",
    "    # Create an empty DataFrame to hold combined data\n",
    "    combined_df = pd.DataFrame()\n",
    "    \n",
    "    # Use glob to find all Excel files in the specified folder that match the pattern\n",
    "    excel_files = glob.glob(os.path.join(folder_path, excel_type))\n",
    "    \n",
    "    # Print found files for debugging\n",
    "    print(f\"Found files: {excel_files}\")\n",
    "    \n",
    "    for file in excel_files:\n",
    "        # Load each Excel file into a DataFrame\n",
    "        df = pd.read_excel(file)\n",
    "        \n",
    "        # Convert all column names to lowercase\n",
    "        df.columns = df.columns.str.lower()\n",
    "        \n",
    "        # Print column names for debugging\n",
    "        #print(f\"Columns in {file}: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Remove specified columns (in lowercase)\n",
    "        columns_to_remove_lower = [col.lower() for col in columns_to_remove]\n",
    "        df.drop(columns=columns_to_remove_lower, inplace=True, errors='ignore')\n",
    "        \n",
    "        # Check if address columns exist before combining them (in lowercase)\n",
    "        existing_address_cols = [col for col in address_cols if col.lower() in df.columns]\n",
    "        \n",
    "        if existing_address_cols:\n",
    "            # Combine address columns into a single column\n",
    "            df['dia chi don vi doi tac'] = df[existing_address_cols].fillna('').agg(' '.join, axis=1)\n",
    "            # Remove the original address columns after combining\n",
    "            df.drop(columns=existing_address_cols, inplace=True, errors='ignore')\n",
    "        else:\n",
    "            print(f\"No address columns found in {file}. Skipping combination.\")\n",
    "        \n",
    "        # Rename headers according to the provided translations (keys should also be in lowercase)\n",
    "        header_translations_lower = {k.lower(): v for k, v in header_translations.items()}\n",
    "        df.rename(columns=header_translations_lower, inplace=True)\n",
    "\n",
    "        # Clean up column names (remove trailing spaces, etc.)\n",
    "        df.columns = df.columns.str.strip()  # Remove leading/trailing spaces\n",
    "        df.columns = df.columns.str.replace(' ', '_')  # Replace spaces with underscores (optional)\n",
    "        df.columns = df.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True) # Remove special characters (optional)\n",
    "        df.columns = df.columns.str.lower() # Convert to lowercase (optional, but recommended)\n",
    "\n",
    "        \n",
    "        # Append the processed DataFrame to the combined DataFrame\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    # Print final combined DataFrame shape and contents for debugging\n",
    "    #print(f\"Final combined DataFrame shape: {combined_df.shape}\")\n",
    "    #print(combined_df.head())  # Display first few rows\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "aa3867f4-49b3-44fa-a854-2dc55c9fd757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files: ['Data_test\\\\Raw Data\\\\72NK.T1.2024test.xlsx', 'Data_test\\\\Raw Data\\\\72NK.T3.2024test.xlsx']\n",
      "Found files: ['Data_test\\\\Raw Data\\\\72XK.T6.2024test.xlsx', 'Data_test\\\\Raw Data\\\\72XK.T8.2021test.xlsx']\n"
     ]
    }
   ],
   "source": [
    "combined_df_NK = combine_excel_files(knowledge_base_path,'72NK*.xlsx', columns_to_remove, address_cols, header_translations)\n",
    "combined_df_XK = combine_excel_files(knowledge_base_path,'72XK*.xlsx', columns_to_remove, address_cols, header_translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3c34eb60-5531-46d9-85f8-e9df505e23eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(combined_df_XK.head())\n",
    "#print(combined_df_NK.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1a3db2aa-0820-4fc0-953b-de6ecc4b5592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_dataframe_to_sql(df, table_name, db_url, if_exists='fail'):\n",
    "    \"\"\"\n",
    "    Inserts a DataFrame into a SQL database table.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The DataFrame to insert.\n",
    "    - table_name: The name of the table in the database.\n",
    "    - db_url: The database connection URL.\n",
    "    - if_exists: What to do if the table already exists. Options are 'fail', 'replace', 'append'.\n",
    "    \"\"\"\n",
    "    # Create a database engine\n",
    "    engine = create_engine(db_url)\n",
    "    \n",
    "    # Insert the DataFrame into the SQL table\n",
    "    df.to_sql(name=table_name, con=engine, if_exists=if_exists, index=False)\n",
    "\n",
    "# Example usage\n",
    "# Define your database URL (replace with your actual credentials)\n",
    "db_url = \"mysql+pymysql://root:\"+Mysqlkey+\"@localhost:3306/my_database\"\n",
    "\n",
    "# Assuming combine_df is your DataFrame that you want to insert\n",
    "insert_dataframe_to_sql(combined_df_NK, 'combined_table_nk', db_url, if_exists='replace')\n",
    "insert_dataframe_to_sql(combined_df_XK, 'combined_table_xk', db_url, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2c155885-9d2b-41ff-b46e-4aeb924d6c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query:  show table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SQL: SELECT * \n",
      "FROM combined_table_nk \n",
      "LIMIT 10;\n",
      "MySQL Error: 1045 (28000): Access denied for user 'root'@'localhost' (using password: YES)\n"
     ]
    }
   ],
   "source": [
    "# Replace with your OpenAI API key\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# Replace with your MySQL credentials\n",
    "db_host = \"localhost\"\n",
    "db_user = \"root\"\n",
    "db_password = os.environ['MYSQL_KEY']\n",
    "db_name = \"my_database\"\n",
    "\n",
    "def execute_sql_from_user_query(user_query):\n",
    "    try:\n",
    "        # 1. Translate user query to SQL using OpenAI Chat Completion\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"\"\"\n",
    "            Translate the following user query into a valid SQL command for MySQL, \n",
    "            suitable for querying tables named 'combined_table_nk' and 'combined_table_xk'.  \n",
    "            Assume the tables contain data related to some combined dataset \n",
    "            (e.g., potentially related by a common field).  \n",
    "            If the user query is ambiguous or could refer to other tables, \n",
    "            make it explicit that the tables are 'combined_table_nk' and 'combined_table_xk'.\n",
    "            The user may ask questions like \"what is the average of x\", \n",
    "            \"what is the sum of y\", or \"show me all the data\". \n",
    "            Be careful to generate efficient SQL, for example, \n",
    "            avoid `SELECT *` unless absolutely necessary.  \n",
    "            If the user asks about specific columns, include them in the select statement. \n",
    "            If the user does not mention columns, and asks to see data, \n",
    "            select a reasonable number of important columns. \n",
    "            If the user asks to see \"all the data\", limit the result to a reasonable number \n",
    "            of rows (e.g., 10) to prevent overwhelming the user.\n",
    "            \"\"\"},\n",
    "            {\"role\": \"user\", \"content\": user_query},\n",
    "        ]\n",
    "\n",
    "        response = openai.chat.completions.create(  # Correct way to call Chat Completion\n",
    "            model=\"gpt-3.5-turbo\",  # Or gpt-4 if you have access\n",
    "            messages=messages,\n",
    "            max_tokens=200,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "\n",
    "        sql_command = response.choices[0].message.content.strip()\n",
    "\n",
    "        print(f\"Generated SQL: {sql_command}\")\n",
    "\n",
    "        # 2. Connect to MySQL (same as before)\n",
    "        mydb = mysql.connector.connect(\n",
    "            host=db_host,\n",
    "            user=db_user,\n",
    "            password=db_password,\n",
    "            database=db_name\n",
    "        )\n",
    "\n",
    "        mycursor = mydb.cursor()\n",
    "\n",
    "        # 3. Execute the SQL command (same as before)\n",
    "        mycursor.execute(sql_command)\n",
    "\n",
    "        # 4. Fetch the results (same as before)\n",
    "        results = mycursor.fetchall()\n",
    "\n",
    "        # 5. Convert results to Pandas DataFrame (same as before)\n",
    "        column_names = [description[0] for description in mycursor.description]\n",
    "        df = pd.DataFrame(results, columns=column_names)\n",
    "\n",
    "        # 6. Close the connection (same as before)\n",
    "        mydb.close()\n",
    "\n",
    "        return df\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        return f\"MySQL Error: {err}\"\n",
    "    except openai.APIError as err:  # Corrected OpenAI error handling!\n",
    "        return f\"OpenAI API Error: {err}\"\n",
    "    except Exception as err:\n",
    "        return f\"An error occurred: {err}\"\n",
    "\n",
    "\n",
    "# Example usage\n",
    "user_query = input(\"Enter your query: \")\n",
    "result = execute_sql_from_user_query(user_query)\n",
    "\n",
    "if isinstance(result, pd.DataFrame):\n",
    "    print(result)\n",
    "elif isinstance(result, str):  # Error message\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6fe173d5-cd1c-4258-ae02-7c11f5ab9b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D@ngklh0407\n"
     ]
    }
   ],
   "source": [
    "print(os.environ['MYSQL_KEYPW'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3665af98-9479-4908-9eac-3715a85a8398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955de2ab-ab44-4d9a-a06e-b49167ab37d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
